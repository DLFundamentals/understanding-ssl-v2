Dataset: cifar100
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169M/169M [00:03<00:00, 46.3MB/s]
Using Decoupled Contrastive Learning
Using Weakly-Supervised Contrastive Learning
[GPU 0] Training epoch 0...
  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                  | 4/48 [00:06<00:55,  1.27s/it]/home/priyadarsimishra/understanding_ssl_v2/utils/optimizer.py:142: UserWarning: This overload of add_ is deprecated:
ðŸ§® Accumulative batch loss at batch idx 0 for DCL model: 8.415793418884277
ðŸ§® Accumulative batch loss at batch idx 0 for NSCL model: 8.405452728271484
ðŸ§® Accumulative batch loss at batch idx 1 for DCL model: 16.916184425354004
ðŸ§® Accumulative batch loss at batch idx 1 for NSCL model: 16.894519805908203
ðŸ§® Accumulative batch loss at batch idx 2 for DCL model: 25.560897827148438
ðŸ§® Accumulative batch loss at batch idx 2 for NSCL model: 25.52774429321289
ðŸ§® Accumulative batch loss at batch idx 3 for DCL model: 34.09453201293945
ðŸ§® Accumulative batch loss at batch idx 3 for NSCL model: 34.05042266845703
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1691.)
  next_v.mul_(momentum).add_(scaled_lr, grad)
ðŸ§® Accumulative batch loss at batch idx 4 for DCL model: 42.44071960449219
ðŸ§® Accumulative batch loss at batch idx 4 for NSCL model: 42.385640144348145
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                         | 20/48 [00:21<00:25,  1.10it/s]
ðŸ§® Accumulative batch loss at batch idx 5 for DCL model: 50.80761528015137
ðŸ§® Accumulative batch loss at batch idx 5 for NSCL model: 50.738858222961426
ðŸ§® Accumulative batch loss at batch idx 6 for DCL model: 59.13401412963867
ðŸ§® Accumulative batch loss at batch idx 6 for NSCL model: 59.0543327331543
ðŸ§® Accumulative batch loss at batch idx 7 for DCL model: 67.30521011352539
ðŸ§® Accumulative batch loss at batch idx 7 for NSCL model: 67.21807289123535
ðŸ§® Accumulative batch loss at batch idx 8 for DCL model: 75.3922758102417
ðŸ§® Accumulative batch loss at batch idx 8 for NSCL model: 75.27351951599121
ðŸ§® Accumulative batch loss at batch idx 9 for DCL model: 83.40083599090576
ðŸ§® Accumulative batch loss at batch idx 9 for NSCL model: 83.28096199035645
ðŸ§® Accumulative batch loss at batch idx 10 for DCL model: 91.37091588973999
ðŸ§® Accumulative batch loss at batch idx 10 for NSCL model: 91.25786542892456
ðŸ§® Accumulative batch loss at batch idx 11 for DCL model: 99.34632110595703
ðŸ§® Accumulative batch loss at batch idx 11 for NSCL model: 99.2552719116211
ðŸ§® Accumulative batch loss at batch idx 12 for DCL model: 107.31814670562744
ðŸ§® Accumulative batch loss at batch idx 12 for NSCL model: 107.16938734054565
ðŸ§® Accumulative batch loss at batch idx 13 for DCL model: 115.24392461776733
ðŸ§® Accumulative batch loss at batch idx 13 for NSCL model: 115.11359548568726
ðŸ§® Accumulative batch loss at batch idx 14 for DCL model: 123.16692018508911
ðŸ§® Accumulative batch loss at batch idx 14 for NSCL model: 123.04691457748413
ðŸ§® Accumulative batch loss at batch idx 15 for DCL model: 131.09726285934448
ðŸ§® Accumulative batch loss at batch idx 15 for NSCL model: 130.97972345352173
ðŸ§® Accumulative batch loss at batch idx 16 for DCL model: 139.03445959091187
ðŸ§® Accumulative batch loss at batch idx 16 for NSCL model: 138.91094636917114
ðŸ§® Accumulative batch loss at batch idx 17 for DCL model: 146.89939832687378
ðŸ§® Accumulative batch loss at batch idx 17 for NSCL model: 146.77190160751343
ðŸ§® Accumulative batch loss at batch idx 18 for DCL model: 154.74247884750366
ðŸ§® Accumulative batch loss at batch idx 18 for NSCL model: 154.5913381576538
ðŸ§® Accumulative batch loss at batch idx 19 for DCL model: 162.55856847763062
ðŸ§® Accumulative batch loss at batch idx 19 for NSCL model: 162.46098136901855
ðŸ§® Accumulative batch loss at batch idx 20 for DCL model: 170.39774370193481
ðŸ§® Accumulative batch loss at batch idx 20 for NSCL model: 170.3133692741394
ðŸ§® Accumulative batch loss at batch idx 21 for DCL model: 178.26371335983276
ðŸ§® Accumulative batch loss at batch idx 21 for NSCL model: 178.16378164291382
ðŸ§® Accumulative batch loss at batch idx 22 for DCL model: 186.1198697090149
ðŸ§® Accumulative batch loss at batch idx 22 for NSCL model: 186.01241874694824
ðŸ§® Accumulative batch loss at batch idx 23 for DCL model: 193.91647005081177
ðŸ§® Accumulative batch loss at batch idx 23 for NSCL model: 193.78908920288086
ðŸ§® Accumulative batch loss at batch idx 24 for DCL model: 201.7083864212036
ðŸ§® Accumulative batch loss at batch idx 24 for NSCL model: 201.5843071937561
ðŸ§® Accumulative batch loss at batch idx 25 for DCL model: 209.56150436401367
ðŸ§® Accumulative batch loss at batch idx 25 for NSCL model: 209.36731386184692
ðŸ§® Accumulative batch loss at batch idx 26 for DCL model: 217.32305717468262
ðŸ§® Accumulative batch loss at batch idx 26 for NSCL model: 217.14957094192505
ðŸ§® Accumulative batch loss at batch idx 27 for DCL model: 225.13884162902832
ðŸ§® Accumulative batch loss at batch idx 27 for NSCL model: 224.8784623146057
ðŸ§® Accumulative batch loss at batch idx 28 for DCL model: 232.92975568771362
ðŸ§® Accumulative batch loss at batch idx 28 for NSCL model: 232.66253852844238
ðŸ§® Accumulative batch loss at batch idx 29 for DCL model: 240.74572134017944
ðŸ§® Accumulative batch loss at batch idx 29 for NSCL model: 240.44396018981934
ðŸ§® Accumulative batch loss at batch idx 30 for DCL model: 248.47450351715088
ðŸ§® Accumulative batch loss at batch idx 30 for NSCL model: 248.19025945663452
ðŸ§® Accumulative batch loss at batch idx 31 for DCL model: 256.24068212509155
ðŸ§® Accumulative batch loss at batch idx 31 for NSCL model: 255.93932580947876
ðŸ§® Accumulative batch loss at batch idx 32 for DCL model: 263.9850778579712
ðŸ§® Accumulative batch loss at batch idx 32 for NSCL model: 263.64618825912476
ðŸ§® Accumulative batch loss at batch idx 33 for DCL model: 271.7656111717224
ðŸ§® Accumulative batch loss at batch idx 33 for NSCL model: 271.38916063308716
ðŸ§® Accumulative batch loss at batch idx 34 for DCL model: 279.5816955566406
ðŸ§® Accumulative batch loss at batch idx 34 for NSCL model: 279.10851669311523
ðŸ§® Accumulative batch loss at batch idx 35 for DCL model: 287.29551696777344
ðŸ§® Accumulative batch loss at batch idx 35 for NSCL model: 286.827440738678
ðŸ§® Accumulative batch loss at batch idx 36 for DCL model: 295.04599475860596
ðŸ§® Accumulative batch loss at batch idx 36 for NSCL model: 294.56570291519165
ðŸ§® Accumulative batch loss at batch idx 37 for DCL model: 302.74654960632324
ðŸ§® Accumulative batch loss at batch idx 37 for NSCL model: 302.27730226516724
ðŸ§® Accumulative batch loss at batch idx 38 for DCL model: 310.4887375831604
ðŸ§® Accumulative batch loss at batch idx 38 for NSCL model: 309.9914445877075
ðŸ§® Accumulative batch loss at batch idx 39 for DCL model: 318.1750011444092
ðŸ§® Accumulative batch loss at batch idx 39 for NSCL model: 317.71093940734863
ðŸ§® Accumulative batch loss at batch idx 40 for DCL model: 325.9162702560425
ðŸ§® Accumulative batch loss at batch idx 40 for NSCL model: 325.4512424468994
ðŸ§® Accumulative batch loss at batch idx 41 for DCL model: 333.6276001930237
ðŸ§® Accumulative batch loss at batch idx 41 for NSCL model: 333.1026258468628
ðŸ§® Accumulative batch loss at batch idx 42 for DCL model: 341.3320622444153
ðŸ§® Accumulative batch loss at batch idx 42 for NSCL model: 340.80177879333496
ðŸ§® Accumulative batch loss at batch idx 43 for DCL model: 349.1165614128113
ðŸ§® Accumulative batch loss at batch idx 43 for NSCL model: 348.5018615722656
ðŸ§® Accumulative batch loss at batch idx 44 for DCL model: 356.8416609764099
ðŸ§® Accumulative batch loss at batch idx 44 for NSCL model: 356.17915439605713
ðŸ§® Accumulative batch loss at batch idx 45 for DCL model: 364.59340047836304
ðŸ§® Accumulative batch loss at batch idx 45 for NSCL model: 363.89885997772217
ðŸ§® Accumulative batch loss at batch idx 46 for DCL model: 372.2874836921692
ðŸ§® Accumulative batch loss at batch idx 46 for NSCL model: 371.5560312271118
ðŸ§® Accumulative batch loss at batch idx 47 for DCL model: 379.92169713974
ðŸ§® Accumulative batch loss at batch idx 47 for NSCL model: 379.2379140853882
Saved model to checkpoints/cifar100_parallel//dcl/snapshot_0.pth at epoch 0
Saved model to checkpoints/cifar100_parallel//nscl/snapshot_0.pth at epoch 0
Saved model at epoch 0
SSL Loss per epoch: 7.915035357077916
NSCL Loss per epoch: 7.90078987677892
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.01it/s]
Evaluation accuracies: [0.056599996984004974]
CDNV: 21.169260025024414, Dir-CDNV: 7.949292182922363
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.55it/s]
Evaluation accuracies: [0.05289999768137932]
CDNV: 21.24757194519043, Dir-CDNV: 8.07400894165039
[GPU 0] Training epoch 1...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:45<00:00,  1.06it/s]
[GPU 0] Training epoch 2...
 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                   | 13/48 [00:14<00:37,  1.08s/it]
Traceback (most recent call last):
  File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 541, in <module>
    trainer.train(epochs)
  File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 263, in train
    ssl_loss_per_epoch, nscl_loss_per_epoch = self._run_epoch(epoch)
  File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 238, in _run_epoch
    self.scaler2.step(self.optimizer2)
  File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 465, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 359, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 359, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 541, in <module>
[rank0]:     trainer.train(epochs)
[rank0]:   File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 263, in train
[rank0]:     ssl_loss_per_epoch, nscl_loss_per_epoch = self._run_epoch(epoch)
[rank0]:   File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 238, in _run_epoch
[rank0]:     self.scaler2.step(self.optimizer2)
[rank0]:   File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 465, in step
[rank0]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank0]:   File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 359, in _maybe_opt_step
[rank0]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank0]:   File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 359, in <genexpr>
[rank0]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank0]: KeyboardInterrupt
