Dataset: cifar100
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169M/169M [00:03<00:00, 47.7MB/s]
Setting up Decoupled Contrastive Learning
Setting up Negatives-only Supervised Contrastive Learning
Setting up Supervised Contrastive Learning
[GPU 0] Training epoch 0...
  4%|â–ˆâ–ˆâ–ˆâ–                                                                              | 2/48 [00:05<01:58,  2.57s/it]/home/priyadarsimishra/understanding_ssl_v2/utils/optimizer.py:142: UserWarning: This overload of add_ is deprecated:
ðŸ§® Accumulative batch loss at batch idx 0 for DCL model: 8.403122901916504
ðŸ§® Accumulative batch loss at batch idx 0 for NSCL model: 8.392017364501953
ðŸ§® Accumulative batch loss at batch idx 0 for SCL model: 8.771510124206543
ðŸ§® Accumulative batch loss at batch idx 1 for DCL model: 16.74514675140381
ðŸ§® Accumulative batch loss at batch idx 1 for NSCL model: 16.722213745117188
ðŸ§® Accumulative batch loss at batch idx 1 for SCL model: 17.54850673675537
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1691.)
  next_v.mul_(momentum).add_(scaled_lr, grad)
ðŸ§® Accumulative batch loss at batch idx 2 for DCL model: 25.210877418518066
ðŸ§® Accumulative batch loss at batch idx 2 for NSCL model: 25.1760892868042
ðŸ§® Accumulative batch loss at batch idx 2 for SCL model: 26.32457447052002
 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                           | 13/48 [00:19<00:42,  1.23s/it]
ðŸ§® Accumulative batch loss at batch idx 3 for DCL model: 33.638628005981445
ðŸ§® Accumulative batch loss at batch idx 3 for NSCL model: 33.59203815460205
ðŸ§® Accumulative batch loss at batch idx 3 for SCL model: 34.92863178253174
ðŸ§® Accumulative batch loss at batch idx 4 for DCL model: 41.89355182647705
ðŸ§® Accumulative batch loss at batch idx 4 for NSCL model: 41.835604667663574
ðŸ§® Accumulative batch loss at batch idx 4 for SCL model: 43.35610294342041
ðŸ§® Accumulative batch loss at batch idx 5 for DCL model: 50.27052974700928
ðŸ§® Accumulative batch loss at batch idx 5 for NSCL model: 50.2029447555542
ðŸ§® Accumulative batch loss at batch idx 5 for SCL model: 51.661415100097656
ðŸ§® Accumulative batch loss at batch idx 6 for DCL model: 58.38678741455078
ðŸ§® Accumulative batch loss at batch idx 6 for NSCL model: 58.29759693145752
ðŸ§® Accumulative batch loss at batch idx 6 for SCL model: 59.89146614074707
ðŸ§® Accumulative batch loss at batch idx 7 for DCL model: 66.42024803161621
ðŸ§® Accumulative batch loss at batch idx 7 for NSCL model: 66.30669784545898
ðŸ§® Accumulative batch loss at batch idx 7 for SCL model: 68.08460521697998
ðŸ§® Accumulative batch loss at batch idx 8 for DCL model: 74.44672203063965
ðŸ§® Accumulative batch loss at batch idx 8 for NSCL model: 74.30012798309326
ðŸ§® Accumulative batch loss at batch idx 8 for SCL model: 76.251296043396
ðŸ§® Accumulative batch loss at batch idx 9 for DCL model: 82.4351396560669
ðŸ§® Accumulative batch loss at batch idx 9 for NSCL model: 82.25813961029053
ðŸ§® Accumulative batch loss at batch idx 9 for SCL model: 84.37986660003662
ðŸ§® Accumulative batch loss at batch idx 10 for DCL model: 90.41500043869019
ðŸ§® Accumulative batch loss at batch idx 10 for NSCL model: 90.23099088668823
ðŸ§® Accumulative batch loss at batch idx 10 for SCL model: 92.52770328521729
ðŸ§® Accumulative batch loss at batch idx 11 for DCL model: 98.36564779281616
ðŸ§® Accumulative batch loss at batch idx 11 for NSCL model: 98.19050788879395
ðŸ§® Accumulative batch loss at batch idx 11 for SCL model: 100.64321994781494
ðŸ§® Accumulative batch loss at batch idx 12 for DCL model: 106.27679586410522
ðŸ§® Accumulative batch loss at batch idx 12 for NSCL model: 106.15003633499146
ðŸ§® Accumulative batch loss at batch idx 12 for SCL model: 108.75638008117676
ðŸ§® Accumulative batch loss at batch idx 13 for DCL model: 114.20351839065552
ðŸ§® Accumulative batch loss at batch idx 13 for NSCL model: 114.0868968963623
ðŸ§® Accumulative batch loss at batch idx 13 for SCL model: 116.85719013214111
ðŸ§® Accumulative batch loss at batch idx 14 for DCL model: 122.0996265411377
ðŸ§® Accumulative batch loss at batch idx 14 for NSCL model: 121.96694564819336
ðŸ§® Accumulative batch loss at batch idx 14 for SCL model: 124.94178104400635
ðŸ§® Accumulative batch loss at batch idx 15 for DCL model: 129.94983386993408
ðŸ§® Accumulative batch loss at batch idx 15 for NSCL model: 129.8488574028015
ðŸ§® Accumulative batch loss at batch idx 15 for SCL model: 133.02087879180908
ðŸ§® Accumulative batch loss at batch idx 16 for DCL model: 137.860698223114
ðŸ§® Accumulative batch loss at batch idx 16 for NSCL model: 137.74615049362183
ðŸ§® Accumulative batch loss at batch idx 16 for SCL model: 141.10199737548828
ðŸ§® Accumulative batch loss at batch idx 17 for DCL model: 145.66954946517944
ðŸ§® Accumulative batch loss at batch idx 17 for NSCL model: 145.54003238677979
ðŸ§® Accumulative batch loss at batch idx 17 for SCL model: 149.16986656188965
ðŸ§® Accumulative batch loss at batch idx 18 for DCL model: 153.50462293624878
ðŸ§® Accumulative batch loss at batch idx 18 for NSCL model: 153.39410877227783
ðŸ§® Accumulative batch loss at batch idx 18 for SCL model: 157.25378513336182
ðŸ§® Accumulative batch loss at batch idx 19 for DCL model: 161.40789413452148
ðŸ§® Accumulative batch loss at batch idx 19 for NSCL model: 161.22266960144043
ðŸ§® Accumulative batch loss at batch idx 19 for SCL model: 165.3150339126587
ðŸ§® Accumulative batch loss at batch idx 20 for DCL model: 169.20359182357788
ðŸ§® Accumulative batch loss at batch idx 20 for NSCL model: 168.99991512298584
ðŸ§® Accumulative batch loss at batch idx 20 for SCL model: 173.384765625
ðŸ§® Accumulative batch loss at batch idx 21 for DCL model: 177.02932500839233
ðŸ§® Accumulative batch loss at batch idx 21 for NSCL model: 176.78236627578735
ðŸ§® Accumulative batch loss at batch idx 21 for SCL model: 181.43359375
ðŸ§® Accumulative batch loss at batch idx 22 for DCL model: 184.8679609298706
ðŸ§® Accumulative batch loss at batch idx 22 for NSCL model: 184.5962986946106
ðŸ§® Accumulative batch loss at batch idx 22 for SCL model: 189.48913383483887
ðŸ§® Accumulative batch loss at batch idx 23 for DCL model: 192.66675901412964
ðŸ§® Accumulative batch loss at batch idx 23 for NSCL model: 192.39842414855957
ðŸ§® Accumulative batch loss at batch idx 23 for SCL model: 197.5347261428833
ðŸ§® Accumulative batch loss at batch idx 24 for DCL model: 200.48703527450562
ðŸ§® Accumulative batch loss at batch idx 24 for NSCL model: 200.21611785888672
ðŸ§® Accumulative batch loss at batch idx 24 for SCL model: 205.58231258392334
ðŸ§® Accumulative batch loss at batch idx 25 for DCL model: 208.18350982666016
ðŸ§® Accumulative batch loss at batch idx 25 for NSCL model: 207.97470569610596
ðŸ§® Accumulative batch loss at batch idx 25 for SCL model: 213.6195936203003
ðŸ§® Accumulative batch loss at batch idx 26 for DCL model: 215.96783542633057
ðŸ§® Accumulative batch loss at batch idx 26 for NSCL model: 215.772292137146
ðŸ§® Accumulative batch loss at batch idx 26 for SCL model: 221.6731767654419
ðŸ§® Accumulative batch loss at batch idx 27 for DCL model: 223.78271770477295
ðŸ§® Accumulative batch loss at batch idx 27 for NSCL model: 223.5453586578369
ðŸ§® Accumulative batch loss at batch idx 27 for SCL model: 229.70689868927002
ðŸ§® Accumulative batch loss at batch idx 28 for DCL model: 231.57244205474854
ðŸ§® Accumulative batch loss at batch idx 28 for NSCL model: 231.33611917495728
ðŸ§® Accumulative batch loss at batch idx 28 for SCL model: 237.75357151031494
ðŸ§® Accumulative batch loss at batch idx 29 for DCL model: 239.3334903717041
ðŸ§® Accumulative batch loss at batch idx 29 for NSCL model: 239.16868114471436
ðŸ§® Accumulative batch loss at batch idx 29 for SCL model: 245.78601169586182
ðŸ§® Accumulative batch loss at batch idx 30 for DCL model: 247.1135025024414
ðŸ§® Accumulative batch loss at batch idx 30 for NSCL model: 246.9652442932129
ðŸ§® Accumulative batch loss at batch idx 30 for SCL model: 253.82267379760742
ðŸ§® Accumulative batch loss at batch idx 31 for DCL model: 254.82118701934814
ðŸ§® Accumulative batch loss at batch idx 31 for NSCL model: 254.64724731445312
ðŸ§® Accumulative batch loss at batch idx 31 for SCL model: 261.84132862091064
ðŸ§® Accumulative batch loss at batch idx 32 for DCL model: 262.6055164337158
ðŸ§® Accumulative batch loss at batch idx 32 for NSCL model: 262.39550971984863
ðŸ§® Accumulative batch loss at batch idx 32 for SCL model: 269.8735179901123
ðŸ§® Accumulative batch loss at batch idx 33 for DCL model: 270.3736095428467
ðŸ§® Accumulative batch loss at batch idx 33 for NSCL model: 270.1879949569702
ðŸ§® Accumulative batch loss at batch idx 33 for SCL model: 277.8957805633545
ðŸ§® Accumulative batch loss at batch idx 34 for DCL model: 278.03389501571655
ðŸ§® Accumulative batch loss at batch idx 34 for NSCL model: 277.9102282524109
ðŸ§® Accumulative batch loss at batch idx 34 for SCL model: 285.9110679626465
ðŸ§® Accumulative batch loss at batch idx 35 for DCL model: 285.75172185897827
ðŸ§® Accumulative batch loss at batch idx 35 for NSCL model: 285.6444296836853
ðŸ§® Accumulative batch loss at batch idx 35 for SCL model: 293.9429578781128
ðŸ§® Accumulative batch loss at batch idx 36 for DCL model: 293.49053287506104
ðŸ§® Accumulative batch loss at batch idx 36 for NSCL model: 293.38552141189575
ðŸ§® Accumulative batch loss at batch idx 36 for SCL model: 301.9669370651245
ðŸ§® Accumulative batch loss at batch idx 37 for DCL model: 301.27121210098267
ðŸ§® Accumulative batch loss at batch idx 37 for NSCL model: 301.13783025741577
ðŸ§® Accumulative batch loss at batch idx 37 for SCL model: 310.0023431777954
ðŸ§® Accumulative batch loss at batch idx 38 for DCL model: 308.9501953125
ðŸ§® Accumulative batch loss at batch idx 38 for NSCL model: 308.82786417007446
ðŸ§® Accumulative batch loss at batch idx 38 for SCL model: 318.0212240219116
ðŸ§® Accumulative batch loss at batch idx 39 for DCL model: 316.65970516204834
ðŸ§® Accumulative batch loss at batch idx 39 for NSCL model: 316.5554494857788
ðŸ§® Accumulative batch loss at batch idx 39 for SCL model: 326.03213119506836
ðŸ§® Accumulative batch loss at batch idx 40 for DCL model: 324.3544964790344
ðŸ§® Accumulative batch loss at batch idx 40 for NSCL model: 324.19428062438965
ðŸ§® Accumulative batch loss at batch idx 40 for SCL model: 334.0471248626709
ðŸ§® Accumulative batch loss at batch idx 41 for DCL model: 332.0671544075012
ðŸ§® Accumulative batch loss at batch idx 41 for NSCL model: 331.92023372650146
ðŸ§® Accumulative batch loss at batch idx 41 for SCL model: 342.0461893081665
ðŸ§® Accumulative batch loss at batch idx 42 for DCL model: 339.73082304000854
ðŸ§® Accumulative batch loss at batch idx 42 for NSCL model: 339.5764408111572
ðŸ§® Accumulative batch loss at batch idx 42 for SCL model: 350.05670642852783
ðŸ§® Accumulative batch loss at batch idx 43 for DCL model: 347.3819727897644
ðŸ§® Accumulative batch loss at batch idx 43 for NSCL model: 347.2823705673218
ðŸ§® Accumulative batch loss at batch idx 43 for SCL model: 358.0775957107544
ðŸ§® Accumulative batch loss at batch idx 44 for DCL model: 355.04482889175415
ðŸ§® Accumulative batch loss at batch idx 44 for NSCL model: 354.9753580093384
ðŸ§® Accumulative batch loss at batch idx 44 for SCL model: 366.1030607223511
ðŸ§® Accumulative batch loss at batch idx 45 for DCL model: 362.75966691970825
ðŸ§® Accumulative batch loss at batch idx 45 for NSCL model: 362.69495964050293
ðŸ§® Accumulative batch loss at batch idx 45 for SCL model: 374.11114597320557
ðŸ§® Accumulative batch loss at batch idx 46 for DCL model: 370.45736360549927
ðŸ§® Accumulative batch loss at batch idx 46 for NSCL model: 370.40142917633057
ðŸ§® Accumulative batch loss at batch idx 46 for SCL model: 382.1206302642822
ðŸ§® Accumulative batch loss at batch idx 47 for DCL model: 378.0709309577942
ðŸ§® Accumulative batch loss at batch idx 47 for NSCL model: 378.0281620025635
ðŸ§® Accumulative batch loss at batch idx 47 for SCL model: 390.10995292663574
Saved dcl model to checkpoints/cifar100_parallel//dcl/snapshot_0.pth at epoch 0
Saved nscl model to checkpoints/cifar100_parallel//nscl/snapshot_0.pth at epoch 0
Saved scl model to checkpoints/cifar100_parallel//scl/snapshot_0.pth at epoch 0
Saved all models at epoch 0
DCL Loss per epoch: 7.876477728287379
NSCL Loss per epoch: 7.875586708386739
SCL Loss per epoch: 8.127290685971579
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.43it/s]
Evaluation accuracies: [0.08020000159740448]
CDNV: 18.227252960205078, Dir-CDNV: 5.658103942871094
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.51it/s]
Evaluation accuracies: [0.07360000163316727]
CDNV: 18.025392532348633, Dir-CDNV: 5.61562967300415
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.68it/s]
Evaluation accuracies: [0.10049999505281448]
CDNV: 17.34691047668457, Dir-CDNV: 4.331777572631836
[GPU 0] Training epoch 1...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:00<00:00,  1.25s/it]
[GPU 0] Training epoch 2...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:00<00:00,  1.26s/it]
[GPU 0] Training epoch 3...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:00<00:00,  1.26s/it]
[GPU 0] Training epoch 4...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:00<00:00,  1.26s/it]
[GPU 0] Training epoch 5...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:00<00:00,  1.26s/it]
[GPU 0] Training epoch 6...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:00<00:00,  1.26s/it]
[GPU 0] Training epoch 7...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:00<00:00,  1.26s/it]
[GPU 0] Training epoch 8...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:00<00:00,  1.26s/it]
[GPU 0] Training epoch 9...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:00<00:00,  1.26s/it]
[GPU 0] Training epoch 10...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:00<00:00,  1.26s/it]
Saved dcl model to checkpoints/cifar100_parallel//dcl/snapshot_10.pth at epoch 10
Saved nscl model to checkpoints/cifar100_parallel//nscl/snapshot_10.pth at epoch 10
Saved scl model to checkpoints/cifar100_parallel//scl/snapshot_10.pth at epoch 10
Saved all models at epoch 10
DCL Loss per epoch: 3.5217980494101844
NSCL Loss per epoch: 3.44124998152256
SCL Loss per epoch: 7.875888526439667
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.60it/s]
Evaluation accuracies: [0.21939998865127563]
CDNV: 3.6273584365844727, Dir-CDNV: 0.2441340684890747
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.64it/s]
Evaluation accuracies: [0.23149999976158142]
CDNV: 3.4765751361846924, Dir-CDNV: 0.22449475526809692
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.64it/s]
Evaluation accuracies: [0.23979999125003815]
CDNV: 10.245842933654785, Dir-CDNV: 0.1913914531469345
[GPU 0] Training epoch 11...
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 23/48 [00:30<00:33,  1.33s/it]
Traceback (most recent call last):
  File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 431, in <module>
    trainer.train(epochs)
  File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 173, in train
    losses_per_epoch = self._run_epoch(epoch)
  File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 151, in _run_epoch
    loss = model_config.train_step(batch, self.gpu_id)
  File "/home/priyadarsimishra/understanding_ssl_v2/models/model_config.py", line 26, in train_step
    self.scaler.step(self.optimizer)
  File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 465, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 360, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/optim/optimizer.py", line 516, in wrapper
    out = func(*args, **kwargs)
  File "/home/priyadarsimishra/understanding_ssl_v2/utils/optimizer.py", line 103, in step
    if p.grad is None:
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 431, in <module>
[rank0]:     trainer.train(epochs)
[rank0]:   File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 173, in train
[rank0]:     losses_per_epoch = self._run_epoch(epoch)
[rank0]:   File "/home/priyadarsimishra/understanding_ssl_v2/scripts/parallel_dcl_nscl_train_simclr.py", line 151, in _run_epoch
[rank0]:     loss = model_config.train_step(batch, self.gpu_id)
[rank0]:   File "/home/priyadarsimishra/understanding_ssl_v2/models/model_config.py", line 26, in train_step
[rank0]:     self.scaler.step(self.optimizer)
[rank0]:   File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 465, in step
[rank0]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank0]:   File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 360, in _maybe_opt_step
[rank0]:     retval = optimizer.step(*args, **kwargs)
[rank0]:   File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/home/priyadarsimishra/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/home/priyadarsimishra/understanding_ssl_v2/utils/optimizer.py", line 103, in step
[rank0]:     if p.grad is None:
[rank0]: KeyboardInterrupt
